# QuickPlay Analytics (Steam Dataset)

**QuickPlay Analytics** — analytics project on Steam games (games, developers, publishers, genres, platforms).
This repo contains SQL schema & transformation scripts, a set of analytic SQL queries, a small Python script to run those queries and export results, and a synthetic-data generator.

> **Important** — I attempted to set up Apache Superset (for dashboards) during the work, but hit environment / dependency issues (missing `flask_cors`, `marshmallow_enum`, and an incompatibility with `marshmallow` that caused Alembic migrations to fail). To finish the assignment reliably I skipped the Superset dashboard part and delivered everything using PostgreSQL (pgAdmin/psql) + Python only. If you want to attempt Superset later, see the *Troubleshooting* section at the end of this README.

---

## Repository layout

```
.
├─ data/steam.csv                 # Original dataset (NOT included - download from Kaggle)
├─ sql/
│  ├─ create_schema.sql           # schema (normalized tables)
│  ├─ create_raw_table.sql        # raw/staging table(s)
│  └─ transform_from_raw.sql      # transforms from raw -> normalized tables
├─ queries/
│  └─ queries.sql                 # 10 analytics queries + basic checks (each query commented)
├─ scripts/
│  └─ generate_synthetic.py       # generate synthetic users / purchases / reviews
├─ main.py                        # run selected queries and export CSV (uses DB connection)
├─ output/                        # CSV results generated by main.py (ignored in git)
├─ screenshot.png                 # placeholder screenshot for analytics
├─ requirements.txt               # Python deps (pandas, sqlalchemy, psycopg2-binary, python-dotenv, ...)
└─ README.md                      # this file
```

**Note:** `data/steam.csv` is *not* included in the repo (Kaggle dataset license / size). Download it and place it at `data/steam.csv` before running the ingestion step.

---

## Prerequisites

* PostgreSQL (e.g., installed with pgAdmin) — tested with a local PostgreSQL server
* Python 3.10+ (create & use a virtual environment)
* `psql` command-line client (or use pgAdmin)
* Git (for pushing to GitHub)

---

## Quickstart — local setup

1. **Create DB user & database (example)**
   Run as `postgres` user (or via pgAdmin):

   ```sql
   -- in psql as postgres:
   CREATE USER steam_user WITH PASSWORD 'steam_pass';
   CREATE DATABASE steamdb OWNER steam_user;
   ```

   Or from shell (PowerShell example):

   ```powershell
   psql -U postgres -c "CREATE USER steam_user WITH PASSWORD 'steam_pass';"
   psql -U postgres -c "CREATE DATABASE steamdb OWNER steam_user;"
   ```

2. **Place the dataset**
   Download the Steam dataset from Kaggle (or your chosen source) and save it as:

   ```
   data/steam.csv
   ```

3. **Create virtual env & install Python deps**

   ```powershell
   python -m venv venv
   .\venv\Scripts\Activate.ps1    # PowerShell
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

   If `requirements.txt` is not present, install minimal packages:

   ```powershell
   pip install pandas sqlalchemy psycopg2-binary python-dotenv
   ```

4. **Load DB schema and staging tables**
   Apply the SQL scripts in this order (adjust file names if your files differ):

   ```powershell
   psql -h localhost -U steam_user -d steamdb -f sql/create_schema.sql
   psql -h localhost -U steam_user -d steamdb -f sql/create_raw_table.sql
   ```

5. **Import CSV into raw table**
   Use `\copy` from your machine (run from repo root):

   ```powershell
   psql -h localhost -U steam_user -d steamdb -c "\copy raw_steam FROM 'data/steam.csv' CSV HEADER;"
   ```

   Replace `raw_steam` with the raw table name defined in `create_raw_table.sql`. If the path or delimiter differs, adjust the command.

6. **Run transform SQL to populate normalized tables**

   ```powershell
   psql -h localhost -U steam_user -d steamdb -f sql/transform_from_raw.sql
   ```

7. **Run analytic queries**

   * To run all queries with `psql`:

     ```powershell
     psql -h localhost -U steam_user -d steamdb -f queries/queries.sql
     ```

   * Or use the Python runner `main.py` which executes a subset / selected queries and exports CSVs to `output/`.

     Set the DB connection string (example SQLAlchemy style) as an environment variable:

     PowerShell (temporary for current session):

     ```powershell
     $env:DATABASE_URL = "postgresql+psycopg2://steam_user:steam_pass@localhost:5432/steamdb"
     python main.py
     ```

     Or create a `.env` with:

     ```
     DATABASE_URL=postgresql+psycopg2://steam_user:steam_pass@localhost:5432/steamdb
     ```

     and `main.py` will read it via `python-dotenv`. Results will be written to `output/` as CSV files.

8. **Generate synthetic data (optional)**

   ```powershell
   $env:DATABASE_URL = "postgresql+psycopg2://steam_user:steam_pass@localhost:5432/steamdb"
   python scripts/generate_synthetic.py
   ```

   The script will insert synthetic users / purchases / reviews into the DB. Review the script header for optional CLI arguments (rows, seed, etc.). If the script expects args, run `python scripts/generate_synthetic.py --help`.

---

## What is included in `queries/queries.sql`

* `SELECT * FROM table LIMIT 10;` checks for base inspection.
* Filtered queries using `WHERE` and sorted with `ORDER BY`.
* Aggregations using `GROUP BY` with `COUNT, AVG, MIN, MAX`.
* At least one `JOIN` between tables (e.g., purchases JOIN users JOIN games).
* **10 analytic themes** covered, each with a short comment describing the metric (for example: top sellers, revenue by month, avg playtime by genre, purchases by platform, reviews per developer, etc.). The file `queries/queries.sql` contains both the SQL and inline comments explaining each query and expected output.

---

## GitHub — upload from local to remote (short guide)

1. Create a repository on GitHub via the web UI ([https://github.com](https://github.com) → New repository). Name: `steam-analytics`.

2. In your local project root, initialize git (if not already):

   ```powershell
   git init
   git add .
   git commit -m "Initial commit: schema, transformation, queries, scripts"
   git branch -M main
   git remote add origin https://github.com/<your-username>/steam-analytics.git
   git push -u origin main
   ```

3. If `origin` already exists and points elsewhere:

   ```powershell
   git remote set-url origin https://github.com/<your-username>/steam-analytics.git
   ```

   And then push:

   ```powershell
   git push -u origin main
   ```

4. **Authentication problems**

   * If Git prompts `please complete authentication in your browser...` but no browser opens, or it times out:

     * Option A: configure Windows credential manager and retry:

       ```powershell
       git config --global credential.helper manager
       git push -u origin main
       ```

       This typically opens a Windows dialog for authentication.
     * Option B: create a **GitHub Personal Access Token (PAT)** (Settings → Developer settings → Personal access tokens → Generate new token) with `repo` scope. Use your GitHub username and the token as the password when `git` prompts.
     * Option C: use GitHub CLI `gh auth login` and follow the interactive flow.

   * If you want to avoid browser flows temporarily (not recommended for permanent storage), you can push with token in URL (be careful — this exposes your token in shell history):

     ```powershell
     git push https://<username>:<PAT>@github.com/<username>/steam-analytics.git
     ```

5. **Common git error**: `error: src refspec main does not match any` → make sure you did `git commit` before pushing.

---

## .gitignore (recommended)

Add a `.gitignore` with at least:

```
venv/
__pycache__/
*.pyc
.env
data/steam.csv         # if you prefer not to include the dataset
output/
*.sqlite
```

Do **not** commit credentials or tokens.

---

## Troubleshooting & notes about Superset (what I tried)

While attempting to add a Superset dashboard, I ran the usual `superset db upgrade` and hit multiple problems:

* Superset refused to start because of missing/unsafe `SECRET_KEY` — fixed by providing `superset_config.py` and setting `SUPERSET_CONFIG_PATH`.
* App failed to create because of missing Python packages: `flask_cors`, `marshmallow_enum` — some of these were fixed by `pip install marshmallow-enum`.
* Later Alembic migrations raised errors (`Field.__init__() got an unexpected keyword argument 'description'`) — a sign of incompatible `marshmallow` version vs Superset code.
* Final migration failed complaining about missing table `ab_user` — environment state inconsistency while attempting migrations.

**Conclusion:** Superset setup requires careful pinning of dependency versions and more time to troubleshoot. For the assignment I proceeded with SQL + Python only. If you'd like, I can produce a follow-up guide to get Superset running (pin versions, create clean virtualenv, follow official Superset install docs).

---

## Final notes (what I completed)

* SQL schema and raw table DDL scripts (in `sql/`)
* Transform script `transform_from_raw.sql` that normalizes the dataset into games, developers, publishers, genres, platforms, etc.
* Ten analytic queries in `queries/queries.sql` (each query has a short comment).
* `scripts/generate_synthetic.py` that simulates users / purchases / reviews.
* `main.py` — Python runner to execute queries and export CSVs to `output/`.
* README (this file) with setup & GitHub instructions.
